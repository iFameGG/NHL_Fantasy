{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d71e07-c753-4a9e-a6ae-4448b7d580d3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd5c7e6-3887-4ff5-8c29-7426fefc8056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.2.2 from /home/fame/anaconda3/lib/python3.9/site-packages/pip (python 3.9)\n",
      "Requirement already satisfied: html5lib in /home/fame/anaconda3/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in /home/fame/anaconda3/lib/python3.9/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/fame/anaconda3/lib/python3.9/site-packages (from html5lib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "!pip install html5lib -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab964b-dff1-44f9-9fa0-fbc1b4f4fe25",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bd958-f881-401a-b50d-601c4f7c565b",
   "metadata": {},
   "source": [
    "## Define SeasonData Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab4946bf-5d29-46f3-8763-334c4df0f03e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SeasonsData:\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "        self.season_tables = {year: self.get_season_tables(year) for year in self.years}\n",
    "        self.season_datasets = {year: self.get_season_datasets(self.season_tables[year]) for year in self.years}\n",
    "        self.complete_dataset =  self.get_complete()\n",
    "        self.shape = self.complete_dataset.shape\n",
    "        \n",
    "    def get_season_tables(self, year):\n",
    "        \"\"\"\n",
    "        Get each table from hockey-reference\n",
    "        season databases\n",
    "        \"\"\"\n",
    "\n",
    "        # -- Define URLS --\n",
    "\n",
    "        ref_url = 'https://www.hockey-reference.com'\n",
    "\n",
    "        basic_ext = f'/leagues/NHL_{year}_skaters.html'\n",
    "        adv_ext = f'/leagues/NHL_{year}_skaters-advanced.html'\n",
    "        shift_ext = f'/leagues/NHL_{year}_skaters-time-on-ice.html'\n",
    "        \n",
    "        #### HAD TO REMOVE MISC BECAUSE IT WASNT TRACKED FOR MOST YEARS\n",
    "        # misc_ext = f'/leagues/NHL_{year}_skaters-misc.html'\n",
    "\n",
    "        url_names = ['basic', 'adv', 'shift',]\n",
    "        url_ext = [basic_ext, adv_ext, shift_ext]\n",
    "\n",
    "        # --- Get Tables ---\n",
    "\n",
    "        raw_tables = {}\n",
    "        print(f'Grabbing {year} season data')\n",
    "        for name, ext in zip(url_names, url_ext):\n",
    "            req = False\n",
    "            fails = 0\n",
    "            while req == False:\n",
    "                sleep_time = 2**fails\n",
    "                try:\n",
    "                    df = pd.read_html(ref_url+ext)[0].apply(lambda x: x.astype(str).str.upper())\n",
    "                    req = True\n",
    "                except:\n",
    "                    fails += 1\n",
    "                    print(fails, 'fails')\n",
    "                    time.sleep(sleep_time)\n",
    "                    pass\n",
    "            li = list(df.columns)\n",
    "            col = [ x[1] for x in li ]\n",
    "            df.columns = col\n",
    "            df = df[df['Rk'] != 'RK'].reset_index(drop=True).sort_values(by='Player')\n",
    "            tot_tm = df[df['Tm'] == 'TOT']\n",
    "            no_dupes = df.drop_duplicates(subset='Player', keep=False)\n",
    "            df = pd.concat([no_dupes, tot_tm])\n",
    "            df_dict = {name: df}\n",
    "            raw_tables.update(df_dict)\n",
    "            print(f'- Grabbed {name}')\n",
    "        \n",
    "        return raw_tables\n",
    "\n",
    "    def get_season_datasets(self, raw_tables):\n",
    "        \"\"\"\n",
    "        Cleans columns of each dataframe and \n",
    "        merges all dataframes into one complete \n",
    "        dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic Dataframe\n",
    "        basic_df = raw_tables['basic'].drop(['FO%'], axis=1)\n",
    "        basic_df.columns =    [\n",
    "                                'Rk', 'Player', 'Age', 'Tm', 'Pos', 'GP', 'G', 'A', 'PTS', '+/-', 'PIM',\n",
    "                                'PS', 'EV_G', 'PP_G', 'SH_G', 'GW_G', 'EV_A', 'PP_A', 'SH_A', 'S', 'S%', 'TOI',\n",
    "                                'ATOI', 'BLK', 'HIT', 'FOW', 'FOL'\n",
    "                            ]\n",
    "        raw_tables['basic'] = basic_df\n",
    "\n",
    "        # Adv Dataframe\n",
    "        adv_df = raw_tables['adv'].drop(['GP', 'Age', 'Rk', 'Pos'], axis=1)\n",
    "        try:\n",
    "            adv_df = adv_df.drop(['E+/-'], axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        adv_df.columns =    [\n",
    "                                'Player', 'Tm', 'CF', 'CA', 'CF%', 'CF% rel',\n",
    "                                'FF', 'FA', 'FF%', 'FF% rel', 'oiSH%', 'oiSV%', 'PDO', 'oZS%', 'dZS%',\n",
    "                                'TOI/60', 'TOI(EV)', 'TK', 'GV', 'SAtt.', 'Thru%'\n",
    "                            ]\n",
    "        raw_tables['adv'] = adv_df\n",
    "\n",
    "        # Shift Dataframe\n",
    "        shift_df = raw_tables['shift'].drop([\n",
    "            'GP', 'Unnamed: 6_level_1', 'Unnamed: 11_level_1', 'Unnamed: 16_level_1', 'Rk', 'Pos'\n",
    "        ], axis=1)\n",
    "        shift_df.columns =  [\n",
    "                                'Player', 'Tm', 'Shift', 'TOI_EVEN',\n",
    "                                'CF% Rel_EVEN', 'GF/60_EVEN', 'GA/60_EVEN', 'TOI_PP', 'CF% Rel_PP',\n",
    "                                'GF/60_PP', 'GA/60_PP', 'TOI_SH', 'CF% Rel_SH', 'GF/60_SH',\n",
    "                                'GA/60_SH'\n",
    "                            ]\n",
    "        raw_tables['shift'] = shift_df\n",
    "        \n",
    "        #### HAD TO REMOVE MISC BECAUSE IT WASNT TRACKED FOR MOST YEARS\n",
    "        # Misc Dataframe\n",
    "        # misc_df = raw_tables['misc'].drop([\n",
    "        #     'GP', 'Age', 'Made', 'Miss', 'Pct.', 'Rk', 'Pos', '+/-', 'PS'\n",
    "        # ], axis=1)\n",
    "        # try:\n",
    "        #     misc_df = misc_df.drop(['xGF', 'xGA', 'E+/-'], axis=1)\n",
    "        # except:\n",
    "        #     pass\n",
    "        # misc_df.columns  =  [\n",
    "        #                         'Player', 'Tm', 'GC', 'G_PG', 'A_PG', 'PTS_PG', 'GC_PG',\n",
    "        #                         'PIM_PG', 'S_PG', 'G_ADJ', 'A_ADJ', 'PTS_ADJ', 'GC_ADJ', 'TGF', 'PGF', \n",
    "        #                         'TGA', 'PGA', 'OPS', 'DPS', 'Att.'\n",
    "        #                     ]\n",
    "        # raw_tables['misc'] = misc_df\n",
    "\n",
    "        # Full Dataframe\n",
    "        full_data = reduce(\n",
    "            lambda  left,right: pd.merge(\n",
    "                left,right,on=['Player', 'Tm'],\n",
    "                how='outer'\n",
    "            ), [ raw_tables[x] for x in raw_tables.keys() ]\n",
    "        ).drop_duplicates(subset='Player', keep='first').dropna(subset=['Pos'])\n",
    "        \n",
    "        full_data = full_data.replace({\n",
    "            'C': 'F', \n",
    "            'LW': 'F', \n",
    "            'RW': 'F', \n",
    "            'W': 'F', \n",
    "            'NAN': np.nan, \n",
    "            '': np.nan\n",
    "        }).dropna()\n",
    "        \n",
    "        # --- Calculate Fantasy Points ---\n",
    "\n",
    "        g_pts = 3\n",
    "        a_pts = 2\n",
    "        pp_pts = 1\n",
    "        sh_pts = 2\n",
    "        hat_pts = 1\n",
    "        sog_pts = 0.1\n",
    "        hts_pts = 0.1\n",
    "        blk = 0.2\n",
    "        \n",
    "        full_data['FTSY_PTS'] = round(sum([\n",
    "            (full_data['G'].astype(int)*g_pts), \n",
    "            (full_data['A'].astype(int)*a_pts), \n",
    "            (full_data['PP_G'].astype(int)*pp_pts), \n",
    "            (full_data['SH_G'].astype(int)*sh_pts), \n",
    "            # (full_data['HAT']*hat_pts), \n",
    "            (full_data['S'].astype(int)*sog_pts), \n",
    "            (full_data['HIT'].astype(int)*hts_pts), \n",
    "            (full_data['BLK'].astype(int)*blk)\n",
    "        ]),1)\n",
    "\n",
    "        return full_data\n",
    "    \n",
    "    def get_sec(self, time_str):\n",
    "        \"\"\"Get seconds from minutes and seconds.\"\"\"\n",
    "        m, s = time_str.split(':')\n",
    "        return float(int(m) * 60 + int(s))\n",
    "    \n",
    "    def get_complete(self):\n",
    "        complete_df = pd.concat(self.season_datasets.values()).drop(['Rk'], axis=1)\n",
    "        \n",
    "        for col in complete_df:\n",
    "            first = complete_df.loc[0, col].tolist()[0]\n",
    "            try:\n",
    "                float(first)\n",
    "                complete_df[col] = complete_df[col].astype(float)\n",
    "            except:\n",
    "                complete_df[col] = complete_df[col].astype(str)\n",
    "        \n",
    "        time_cols = ['ATOI', 'TOI/60', 'TOI(EV)', 'Shift', 'TOI_EVEN', 'TOI_PP', 'TOI_SH']\n",
    "        for col in time_cols:\n",
    "            complete_df[col] = complete_df[col].apply(lambda time_str: self.get_sec(time_str))\n",
    "        \n",
    "        return complete_df.reset_index(drop=True)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.complete_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5f347-093f-4bc8-804f-8494303ba3ce",
   "metadata": {},
   "source": [
    "## Initialize SeasonData Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753976a6-fd2d-43f9-b20c-54a47cf2d5c8",
   "metadata": {},
   "source": [
    "Getting player data from 2008 to the present day would be the ideal dataset, but I keep getting an error:\n",
    "``` Python\n",
    "HTTPError: HTTP Error 429: Too Many Requests\n",
    "```\n",
    "To get arround this, I will be using a smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bc704-4ce1-4b3d-9fda-416f90c60fde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing 2014 season data\n",
      "1 fails\n",
      "2 fails\n",
      "3 fails\n"
     ]
    }
   ],
   "source": [
    "years = list(range(2014, datetime.date.today().year))\n",
    "data = SeasonsData(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bce0c-386e-439a-871a-dcfb075a97c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Attributes and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d356c1-0695-4e44-b014-a42dfb76cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09be4bf-2f5e-42fb-98a1-786e261310f1",
   "metadata": {},
   "source": [
    "59 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b30b83-e2f8-4083-ab71-d4f76eeb881a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.complete_dataset.query('Player == \"CONNOR MCDAVID\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040974e8-402c-4673-b588-8547da319eb6",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ddae4-6076-401a-9345-3f2964ef7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.complete_dataset\n",
    "\n",
    "# Inputs\n",
    "X = dataset.drop(['Player', 'Tm'], axis=1)\n",
    "\n",
    "# Output\n",
    "y = dataset['FTSY_PTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125353a8-e1f1-4305-b748-2f27435d1adb",
   "metadata": {},
   "source": [
    "## Preprocess X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51825129-9e3f-4ff8-9e94-00115b23753e",
   "metadata": {},
   "source": [
    "### Drop Variables associated with FTSY_PTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02065db1-b961-4c10-8192-671232e304d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    'G', 'A', 'PP_G','SH_G', 'GW_G', 'EV_G', 'PS',\n",
    "    'PP_A', 'SH_A', 'BLK','HIT', 'EV_A', 'PTS'\n",
    "]\n",
    "X = X.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce8857-5ac7-4615-b4fe-4b41977a52ef",
   "metadata": {},
   "source": [
    "### Get Dummies For Pos Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f66bf-8f38-4f4c-8fcf-d434c333bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posd = pd.get_dummies(X.Pos)\n",
    "X = pd.concat([X, posd],axis=1).drop(['Pos'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beeca1f-408f-4aaf-aaf0-310e8f08cba2",
   "metadata": {},
   "source": [
    "### Add Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137fdc6-4269-4f6e-8b5d-2a94b70d908b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "degree = 2\n",
    "for col in X:\n",
    "    X[col+f'_{degree}'] = X[col] ** degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed7c5a-acd0-4b1c-bafb-aab13cfd2ae7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Drop Highly Correlated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056aba30-95d1-4e6b-98c5-051c7fb24eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr().abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4ab5e-ac20-44af-8794-e18aee907546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated values\n",
    "cor_matrix = X.corr().abs()\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] >= 0.8)]\n",
    "X = X.drop(to_drop, axis=1)\n",
    "\n",
    "print('Dropped: \\n', to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7df5dc-a54c-40f6-842d-15bacb6ed9dd",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f6f2c-878e-42e2-9874-163dfb42385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = preprocessing.normalize(X)\n",
    "X = pd.DataFrame(X_norm, columns =list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb093428-cdde-4fd2-9bb2-f987382a18ce",
   "metadata": {},
   "source": [
    "## Preprocess Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d2a58-93ed-4c20-8ffd-7ce43e78d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.hist(y)\n",
    "ax1.set_title('Not Log Treated')\n",
    "ax2.hist(np.log(y))\n",
    "ax2.set_title('Log Treated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadb492-942b-4010-af31-dce0c7413ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b5c86-7a2f-471a-9342-e786fc01fca2",
   "metadata": {},
   "source": [
    "# OLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5de62-8496-4c6e-ad1e-3149db14bef2",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838bc49-4c83-4d5b-9e24-9164da7c8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a97edf-d3a5-4522-b67a-cfac6ebdac0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1a6eb-a15d-4d57-a876-50e880e5b6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ols = sm.OLS(y_train, sm.add_constant(X_train)).fit(cov_type='HC3')\n",
    "model_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36813a-05e9-4d42-8e58-34db73d72790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model_ols.predict(sm.add_constant(X_test))\n",
    "\n",
    "y_test_real = np.exp(y_test)\n",
    "y_pred_real = np.exp(y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "mse = mean_squared_error(y_test_real, y_pred_real)\n",
    "rmse = mean_squared_error(y_test_real, y_pred_real, squared=False)\n",
    "\n",
    "print(f\"Mean Absolute Error: {round(mae, 2)}\")\n",
    "print(f\"Mean Squared Error: {round(mse, 2)}\")\n",
    "print(f\"Root Mean Squared Error: {round(rmse, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535eef2a-6083-490c-a281-ceb5e814b7c3",
   "metadata": {},
   "source": [
    "# WLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71319f-fc77-4a8c-914d-b2b0ed5c377d",
   "metadata": {},
   "source": [
    "## Calculate Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fee97-a2a8-46e8-9807-f05d39ca2583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5, figsize=(10,10))\n",
    "\n",
    "cols = X.columns\n",
    "col_idx = 0\n",
    "for i1 in range(5):\n",
    "    for i2 in range(5):\n",
    "        ax[i1,i2].scatter(X[cols[col_idx]], y)\n",
    "        ax[i1,i2].set_title(cols[col_idx])\n",
    "        col_idx+=1\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c046-ef5b-4e29-99c1-41e9f5765433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = 1 / smf.ols('model_ols.resid.abs() ~ model_ols.fittedvalues', data=X_train).fit().fittedvalues**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51fe0c-0d0b-4bfc-a547-f8ceb4c4192d",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b3223-c310-4a2b-89ed-f90679caf912",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wls = sm.WLS(y_train, sm.add_constant(X_train), weights=weights).fit()\n",
    "model_wls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135ddf4-b2ec-49f9-8a05-04da25765455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model_wls.predict(sm.add_constant(X_test))\n",
    "\n",
    "y_test_real = np.exp(y_test)\n",
    "y_pred_real = np.exp(y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "mse = mean_squared_error(y_test_real, y_pred_real)\n",
    "rmse = mean_squared_error(y_test_real, y_pred_real, squared=False)\n",
    "\n",
    "print(f\"Mean Absolute Error: {round(mae, 2)}\")\n",
    "print(f\"Mean Squared Error: {round(mse, 2)}\")\n",
    "print(f\"Root Mean Squared Error: {round(rmse, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57377e35-6562-4770-8c33-6345898a94f9",
   "metadata": {},
   "source": [
    "# Optimized WLS *(Insignificant Values Removed)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457d59a-7c80-43d8-adff-274f5558675b",
   "metadata": {},
   "source": [
    "## Remove Insignificant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14e8c8-0658-4aad-aa30-0078a6bb09ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_drop = (\n",
    "    list(model_wls.pvalues[model_wls.pvalues > 0.05].index)\n",
    "                           +\n",
    "    ['Shift', 'ATOI', 'TOI_SH', 'GF/60_PP_2', 'GA/60_PP']\n",
    ")\n",
    "X_opt = X.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "print('Dropped: \\n', to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ac480-6198-480d-aad8-e5955a743a0b",
   "metadata": {},
   "source": [
    "## Split New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95b716-9af1-45b0-a8da-1ae9ed3716a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_opt, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cefb9-c277-488e-8812-e001bf345142",
   "metadata": {},
   "source": [
    "## Recalculate Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e3e45-2888-44dd-8c96-4a10327a1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ols_opt = sm.OLS(y_train, sm.add_constant(X_train)).fit(cov_type='HC3')\n",
    "weights = 1 / smf.ols('model_ols_opt.resid.abs() ~ model_ols_opt.fittedvalues', data=X_train).fit().fittedvalues**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4f3d7-1a70-4caa-83ec-da69ad853072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_opt = sm.WLS(y_train, sm.add_constant(X_train), weights=weights).fit()\n",
    "model_opt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f2ed7-46f3-4c38-b88e-1f573c52397b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model_opt.predict(sm.add_constant(X_test))\n",
    "\n",
    "y_test_real = np.exp(y_test)\n",
    "y_pred_real = np.exp(y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "mse = mean_squared_error(y_test_real, y_pred_real)\n",
    "rmse = mean_squared_error(y_test_real, y_pred_real, squared=False)\n",
    "\n",
    "print(f\"Mean Absolute Error: {round(mae, 2)}\")\n",
    "print(f\"Mean Squared Error: {round(mse, 2)}\")\n",
    "print(f\"Root Mean Squared Error: {round(rmse, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535efab-c55e-4120-bfb3-c1a5d905b205",
   "metadata": {},
   "source": [
    "# Model Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e64a1-22db-4705-ba40-05eef6fb2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_wls = model_wls.pvalues.to_frame().reset_index().rename({'index': 'Models', 0:'WLS Model'}, axis=1).round(2)\n",
    "p_opt = model_opt.pvalues.to_frame().reset_index().rename({'index': 'Models', 0:'OPT Model'}, axis=1).round(2)\n",
    "\n",
    "p_table = p_wls.merge(p_opt, on='Models', how='left').sort_values(by='OPT Model').set_index('Models').T.reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,0.3))\n",
    "ax.table(cellText=p_table.values, colLabels=p_table.columns)\n",
    "ax.axis('off')\n",
    "ax.set_title('P-Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4d88c-77af-492f-8bd7-3d3b604b0df1",
   "metadata": {
    "tags": []
   },
   "source": [
    "| Metrics  | WLS Model  | Optimized WLS Model  |\n",
    "|---|---|---|\n",
    "| MSE  | <span style=\"color: green;\">11.43</span>  | <span style=\"color: red;\">14.17</span>  | \n",
    "| RMSE  | <span style=\"color: green;\">16.11</span> | <span style=\"color: red;\">19.88</span>  | \n",
    "| R^2  | <span style=\"color: green;\">0.924</span>  | <span style=\"color: red;\">0.885</span>  | \n",
    "----------------------------\n",
    "*Note: The WLS Model has a many values that are statistically insignificant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21cfa32-80bb-4485-8c14-471b3cef3997",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f00f9-0623-4f81-8228-9731ea999bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
